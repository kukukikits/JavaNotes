# GPU资源
## 10.0.40.92服务器说明
1. ffmpeg版本为 3.4.8，源码地址为/home/user/ffmpeg_sources/ffmpeg-3.4.8


# ffmpeg使用NVIDIA GPU编解码H.264/rtsp流
前提是系统有NVIDIA GPU
1. 首先安装NVIDIA video codec sdk
   见https://docs.nvidia.com/video-technologies/video-codec-sdk/ffmpeg-with-nvidia-gpu/index.html#hardware-setup
    这里安装sdk n8.1.24.12版本
    ```sh
    git clone https://git.videolan.org/git/ffmpeg/nv-codec-headers.git
    # 切换到n8.1.24.12 TAG
    git checkout n8.1.24.12
    cd nv-codec-headers && sudo make install
    ```
2. 下载安装ffmpeg 3.4.8版本，因为4.3.2(写时最新)的版本要求NVIDIA video codec sdk的最低版本为11
   1. ffmpeg 3.4.8版本下载地址：https://ffmpeg.org/releases/ffmpeg-3.4.8.tar.bz2
   2. 根据https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu的步骤安装好ffmpeg的其他依赖库，最后可以使用如下命令编译和安装：
   ```sh
    PATH="$HOME/bin:$PATH" PKG_CONFIG_PATH="$HOME/ffmpeg_build/lib/pkgconfig" ./configure \
   --prefix="$HOME/ffmpeg_build" \
   --pkg-config-flags="--static" \
   --extra-cflags="-I$HOME/ffmpeg_build/include" \
   --extra-ldflags="-L$HOME/ffmpeg_build/lib" \
    --extra-libs="-lpthread -lm" \
    --bindir="$HOME/bin" \
   --enable-gpl \
   --enable-gnutls \
   --enable-libass \
   --enable-libfdk-aac \
   --enable-libfreetype \
   --enable-libmp3lame --enable-libopus   \
   --enable-libvorbis --enable-libvpx --enable-libx264 --enable-nonfree \
   --enable-cuda-sdk --enable-cuvid --enable-nvenc \
   --enable-libnpp --extra-cflags=-I/usr/local/cuda/include --extra-ldflags=-L/usr/local/cuda/lib64 && \
   PATH="$HOME/bin:$PATH" make -j 10 && \
   make install && \
   hash -r
   ```
    所有编译时支持的option（如--enable-cuda-sdk ）都在ffmpeg工程的configure文件里有说明，有些配置项在不同的ffmpeg版本里不一样，比如--enable-cuda-sdk在ffmpeg 4.3.2中就已经弃用了
3. 安装程序依赖：
   ```sh
    pip install requests redis ffmpeg numpy opencv-python
   ```
4. 使用ffmpeg拉流，并在GPU中解码。
   下面的程序主要用于在服务器上运行，程序使用ffmpeg从流媒体服务器上拉取rtsp流，然后使用GPU解码H.264格式的流，然后在CPU中转换为bgr24的格式。其他程序如tensorflow、opencv等图片处理程序就可以使用bgr24格式的图片进行运算，然后把运算后输出的新图片重新以rtsp流的形式推流到流媒体服务器上。

   下面是示例程序
   ```py
   #coding=utf-8
   import cv2
   import time
   import subprocess as sp
   import requests
   import json
   import platform
   import threading
   import signal
   import sys
   import traceback
   import redis
   import queue
   import struct
   import ffmpeg
   import numpy as np
   from threading import Event
   # https://www.dazhuanlan.com/2019/12/16/5df679224a5d1/

   # 线程共享全局变量
   currentSourceFrame = None
   currentResultFrame = None
   rc = redis.StrictRedis(host='x.x.x.x', port='6379', db=3, password='xxxx')

   class VideoCapture:
     def __init__(self, name):
       self.cap = cv2.VideoCapture(name)
       self.q = queue.Queue()
       t = threading.Thread(target=self._reader)
       t.daemon = True
       t.start()

     # read frames as soon as they are available, keeping only most recent one
     def _reader(self):
       while True:
         ret, frame2 = self.cap.read()
         if not ret:
           break
         if not self.q.empty():
           try:
             self.q.get_nowait()   # discard previous (unprocessed) frame
           except queue.Empty:
             pass
         self.q.put(frame2)

     def read(self):
       return self.q.get()

   def packImage(img, timestamp=None):
       h, w = img.shape[:2]
       shape = struct.pack('>II', h, w)
       img_time = struct.pack('>d', timestamp or time.time())
       encoded = shape + img_time + img.tobytes()
       return encoded

   def packImageBytes(imageBytes, w, h, timestamp=None):
       shape = struct.pack('>II', h, w)
       img_time = struct.pack('>d', timestamp or time.time())
       encoded = shape + img_time + imageBytes
       return encoded

   def printException(Exception, e):
       print('str(Exception):\t', str(Exception))
       print('str(e):\t\t', str(e))
       print('repr(e):\t', repr(e))
       # Get information about the exception that is currently being handled  
       exc_type, exc_value, exc_traceback = sys.exc_info() 
       print('e.message:\t', exc_value)
       print("Note, object e and exc of Class %s is %s the same." % 
                 (type(exc_value), ('not', '')[exc_value is e]))
       print('traceback.print_exc(): ', traceback.print_exc())
       print('traceback.format_exc():\n%s' % traceback.format_exc())
       print('########################################################')

   class ImageReadingThread (threading.Thread):
       def __init__(self, ffmpegPath, sourceStream, rtspUrl, rtspHttpServer, frameEvent):
           threading.Thread.__init__(self)
           self.setDaemon(True)
           self.name = "ImageReadingThread"
           self.ffmpegPath = ffmpegPath
           self.sourceStream = sourceStream
           self.rtspUrl = rtspUrl
           self.rtspHttpServer = rtspHttpServer
           self.camera = None
           self.pushingThread = None
           self.frameEvent = frameEvent
       def run(self):
           while True:
               try:
                   self.readSource()
               except Exception as e:
                   printException(Exception, e)
                   print(self.name + ' catch exception. Sleep 10s then retry.')
                   time.sleep(10)  
       def readGPU(self, w=640, h=480):
           # https://medium.com/@fanzongshaoxing/use-ffmpeg-to-decode-h-264-stream-with-nvidia-gpu-acceleration-16b660fd925d
           # https://stackoverflow.com/questions/59999453/ffmpeg-rtsp-streams-to-rgb24-using-gpu
           # https://stackoverflow.com/questions/16658873/how-to-minimize-the-delay-in-a-live-streaming-with-ffmpeg
           # https://docs.nvidia.com/video-technologies/video-codec-sdk/ffmpeg-with-nvidia-gpu/index.html#hardware-setup
           # https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu
           process = (
               ffmpeg
               # 在GPU中解码
               .input(self.sourceStream, vsync=0, fflags='nobuffer', flags='low_delay', strict='experimental', hwaccel='cuvid', vcodec='h264_cuvid', rtsp_transport='tcp',allowed_media_types='video',max_delay="500000", reorder_queue_size="10000", probesize=32,analyzeduration=0)
               # hwdownload从GPU中把图片加载到内存，然后使用CPU转将图片转为bgr24
               .output('pipe:', format='rawvideo', vf='hwdownload,format=nv12', pix_fmt='bgr24')
               .run_async(pipe_stdout=True)
           )

           while True:
               in_bytes = process.stdout.read(int(w*h*3))
               if not in_bytes:
                   break
               self.on_data(in_bytes, w, h)

       def on_data(self, in_bytes, w, h):
           # 使用redis查看实时流，用于测试
           # rc.publish("living_stream", packImageBytes(in_bytes, w, h))
           global currentSourceFrame
           # 不知道这里reshape h 和 w的顺序对不对
           frame = np.frombuffer(in_bytes, dtype=np.uint8).reshape(h, w, 3)
           currentSourceFrame = frame
           self.frameEvent.set() #--> 发送事件

       def readSource(self):
           # 如果是流媒体，从服务器监测指定流媒体sourceStream是否存在
           if self.sourceStream.startswith('rtsp') or self.sourceStream.startswith('rtmp'):
               while self.preCheckConnection(self.rtspHttpServer, self.sourceStream) != 0:
                   print('Can\'t get sourceStream information. Wait for 10 seconds then retry ')
                   time.sleep(10)
           if self.camera:
               self.camera.release()
               self.camera = None

           # 从sourceStream读取视频。因为发现使用下面的方法拉取rtsp流有很高的延迟（可能是opencv在解码H.264格式的rtsp流时使用的是CPU，解码性能不够）。因此这里仅仅使用下面的方法从rtsp流中获取必要的视频属性数据，如大小，帧率
           videoCapture = VideoCapture(sourceStream)
           #camera = cv2.VideoCapture(sourceStream) 
           camera = videoCapture.cap
           #camera.set(cv2.CAP_PROP_BUFFERSIZE, 3)
           
           self.camera = camera
           if not camera.isOpened():
               print('rtsp stream is not available')
               return
           
           # 视频属性
           size = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)), int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))
           if size[0] <= 0 or size[1] <= 0:
               print('frame can\'t be 0x0')
               return
           sizeStr = str(size[0]) + 'x' + str(size[1])
           fps = camera.get(cv2.CAP_PROP_FPS)  # 30p/self
           fps = int(fps)
           if fps < 0 or fps > 60:
               print('Can\'t get fps of stream, using default 25 fps')
               fps = 25
           hz = int(1000.0 / fps)
           print('size:'+ sizeStr + ' fps:' + str(fps) + ' hz:' + str(hz))
           self.startPushingThread(self.ffmpegPath, sizeStr, fps, self.rtspUrl, self.frameEvent, self.rtspHttpServer)

           camera.release()
           self.readGPU(size[0], size[1])

           # global currentSourceFrame
           # while True:
           #     ret, frame = camera.read() # 逐帧采集视频流
           #     if ret:
           #         currentSourceFrame = frame
           #         rc.publish("living_stream", packImage(frame))
           #         self.frameEvent.set() #--> 发送事件
           #     else:
           #         currentSourceFrame = None
           #         return
       def startPushingThread(self, ffmpegPath, size, fps, rtspUrl, frameEvent, rtspHttpServer):
           if self.pushingThread:
               return
           self.pushingThread = ResultPushingThread(ffmpegPath, size, fps, rtspUrl, frameEvent, rtspHttpServer)
           self.pushingThread.setDaemon(True)
           self.pushingThread.start()
       def preCheckConnection(self, rtspHttpServer, sourceStream):
           queries = sourceStream.split('/')
           query = queries[len(queries) - 1]
           requestUrl = rtspHttpServer + "/api/v1/pushers?q=" + query
           print('Request url: ', requestUrl)
           res = requests.get(requestUrl)
           body = json.loads(res.text)
           print('Rtsp http server response: ', body)
           if res and res.status_code == 200 and body['total'] >= 1:
               for row in body['rows']:
                   if row['source'].endswith(sourceStream):
                       return 0
               return -1
           else:
               return -1

   class ImageProcessThread(threading.Thread):
       def __init__(self, frameEvent):
           threading.Thread.__init__(self)
           self.name = "ImageProcessThread"
           self.setDaemon(True)
           self.frameEvent = frameEvent
       def run(self):
           while True:
               try:
                   self.process()
               except Exception as e:
                   printException(Exception, e)
                   print(self.name + ' catch exception. Sleep 10s then retry.')
                   time.sleep(10)

       def process(self):
           global currentSourceFrame
           global currentResultFrame
           while True:
               # 将线程共享变量保存到本地变量frame上，使用frame作为程序输入
               frame = currentSourceFrame
               if not frame is None:
                   # 模拟处理图片的过程
                   # 处理图片中
                   time.sleep(1/15)
                   currentResultFrame = frame
               else:
                   time.sleep(10)    
          
   class ResultPushingThread (threading.Thread):
       def __init__(self, ffmpegPath, size, fps, rtspUrl, frameEvent, rtspHttpServer):
           threading.Thread.__init__(self)
           self.setDaemon(True)
           self.name = "ResultPushingThread"
           self.ffmpegPath = ffmpegPath
           self.size = size
           self.fps = fps
           self.rtspUrl = rtspUrl
           self.pipe = None
           self.frameEvent = frameEvent
           self.rtspHttpServer = rtspHttpServer
           self.streamInfo = {}
           self.isStop = False
       def stop(self):
           self.isStop = True
           queries = self.rtspUrl.split('/')
           query = queries[len(queries) - 1]
           requestUrl = self.rtspHttpServer + "/api/v1/pushers?q=" + query
           print('Query stream info: ', requestUrl)
           res = requests.get(requestUrl)
           body = json.loads(res.text)
           print('Rtsp http server response: ', body)
           if res and res.status_code == 200 and body['total'] >= 1:
               for row in body['rows']:
                   if row['source'] == self.rtspUrl:
                       self.streamInfo = row
                       break
           if self.streamInfo and self.streamInfo['id']:
               requestUrl = self.rtspHttpServer + "/api/v1/stream/stop?id=" + self.streamInfo['id']
               print('Stop pushing stream request: ', requestUrl)
               res = requests.get(requestUrl)
               print('Rtsp http server response: ', res.status_code, ' body:', res.text)
       def run(self): 
           while not self.isStop:
               try:
                   self.push()
               except Exception as e:
                   printException(Exception, e)
                   print(self.name + ' catch exception. Sleep 10s then retry.')
                   time.sleep(10)    
               finally:
                   if self.pipe:
                       self.pipe.terminate()
                       self.pipe = None    
       def push(self):
           # 直播管道输出 
           # ffmpeg推送rtsp 重点 ： 通过管道 共享数据的方式
           command = [self.ffmpegPath,
               '-y',
               '-f', 'rawvideo',
               '-vcodec','rawvideo',
               '-pix_fmt', 'bgr24',
               '-s', self.size,
               '-r', str(self.fps), # 默认fps是25
               '-i', '-',
                # 以上为对视频流输入的参数设置

                # 以下为输出
               # https://github.com/arut/nginx-rtmp-module/issues/378#issuecomment-323592252
               '-c:v', 'libx264',
               '-b:v', '500k', # 码率
               '-crf', '30',   # 0 to 63, 数字越大表示质量越低，输出大小越小
               '-intra-refresh', '1',
               # https://stackoverflow.com/questions/43868982/low-latency-dash-nginx-rtmp/45370210#45370210
               '-g', '50',     # 设置关键帧间隔， The default GOP size for ffmpeg is 250 which means there will be a key frame every 250 frames.
               '-pix_fmt', 'yuv420p',
               '-preset', 'ultrafast',
               '-tune', 'zerolatency',
               
               # "-maxrate", "750k",
               # "-bufsize", "3000k",
               # "-movflags", "+faststart",
               # "-x264opts", "opencl",
               
               # 点对点传输
               # "-f", "mpegts", "udp://xx.xx.xx.xx:2000"

               # rtsp推流
               '-rtsp_transport', 'tcp',
               '-f', 'rtsp', self.rtspUrl,
           ]

           if self.pipe:
               self.pipe.terminate()
           #管道特性配置
           self.pipe = sp.Popen(command, stdin=sp.PIPE)   

           global currentResultFrame
           while True:
               self.frameEvent.wait() # 等待图像处理结果
               frame = currentResultFrame
               if not frame is None:
                   # 用ffmpeg工具将结果帧推流到rtsp流媒体服务器
                   self.pipe.stdin.write(frame.tobytes())  # 存入管道用于直播
               else:
                   time.sleep(10)
               self.frameEvent.clear() #将event的标志设置为False，调用wait方法的所有线程将被阻塞； 


   # 替换ffmpeg的路径
   system = platform.system()
   if system == "Windows":
       ffmpegPath = "D:/Programs/ffmpeg-4.3.1-2021-01-26-full_build/bin/ffmpeg.exe"
   elif system == "Linux":
       ffmpegPath = "ffmpeg"

   # 使用Easydarwin流媒体服务器
   sourceStream = 'rtsp://xx.xx.xx.xx:554/stream' # 源视频流地址
   rtspHttpServer = 'http://xx.xx.xx.xx:10008'
   rtspUrl = 'rtsp://xx.xx.xx.xx:554/test.result' # 处理完后视频后，将图片处理结果推流到这个地址
   pushingThread = None
   def exitHandler(signum, frame):
       if not pushingThread is None:
           pushingThread.stop()
       exit()

   def run():
       
       frameEvent = Event()
       readingThread = ImageReadingThread(ffmpegPath, sourceStream, rtspUrl, rtspHttpServer, frameEvent)
       processThread = ImageProcessThread(frameEvent)
       readingThread.setDaemon(True)
       processThread.setDaemon(True)
       readingThread.start()
       processThread.start()
       signal.signal(signal.SIGINT, exitHandler)
       signal.signal(signal.SIGTERM, exitHandler)

       global pushingThread
       while True:
           if pushingThread is None:
               pushingThread = readingThread.pushingThread
           pass
       
   run()

   ```


5. 可以使用下面的程序测试视频延时
```py
import cv2
import redis
import struct
import numpy as np

def unpackImage(encoded):
     h, w = struct.unpack('>II', encoded[:8])
     img_time = struct.unpack('>d', encoded[8:16])
     img = np.frombuffer(encoded, dtype=np.uint8, offset=16).reshape(h, w, 3)
     return img, img_time[0]

# 使用Redis预览画面
def redisRun():
    rc = redis.StrictRedis(host='xx.xx.xx.xx', port='36379', db=3, password='xx')
    ps = rc.pubsub()
    ps.subscribe('living_stream')  #从liao订阅消息
    print("started")
    for item in ps.listen():		#监听状态：有消息发布了就拿过来
        if item['type'] == 'message':
            img, img_time = unpackImage(item['data'])
            cv2.imshow('frame', img)
            if cv2.waitKey(20) & 0xFF == ord('q'):
                break

# 使用opencv预览画面（windows上可能比实时，最好和redisRun预览方法对比一下，因为发现在服务器上VideoCapture拉取rtsp流有很高的延迟）
def cvRun():
    cap = cv2.VideoCapture("rtsp://xx.xx.xx.xx:554/uvc.result")
    while(cap.isOpened()):
        ret, frame = cap.read()
        cv2.imshow('frame', frame)
        if cv2.waitKey(20) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()

# 下面两个方法不能同时运行
cvRun()
#redisRun()        

```