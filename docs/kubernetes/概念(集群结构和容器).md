# Concepts

## Cluster Architecture 集群结构

### Control Panel - Node Communication 控制面板和节点间的通讯
#### 1. Node to Control Panel 节点到控制面板
Kubernetes具有一个“hub-and-spoke”（中心辐射）型的API模式。所有API使用始于节点，终于apiserver(Control plane没有其他被用来暴露远程服务的组件)。Apiserver监听HTTPS 443端口，并且使用一种或多种形式的client authentication机制。在匿名访问和service account tokens开启的情况下，应该使用一种或多种形式的authorization机制。

应该为节点配置集群的公共根证书，这样节点就可以通过携带client credentials安全地连接到apiserver。

如果Pods需要连接到apiserver，那么可以使用service account，这样kubernetes就会在Pods实例化的时候自动注入公共根证书和合法的bearer token到Pods里面。Kubernetes service配置了虚拟的IP地址，该地址被kube-proxy重定向到apiserver的HTTPS端口

Control plane组件同样地通过安全端口与集群apiserver通信。

结果就是，在默认的操作模式下，节点和pods到control plane的连接默认是安全的，可以在不可信/公共网络中使用。

#### 2. Control Plane to node 控制面板到节点
从控制平面（apiserver）到节点有两条主要的通信路径。第一个是从apiserver到kubelet进程，该进程在集群中的每个节点上运行。第二种是通过apiserver的代理功能从apiserver到任何节点、pod或服务。
##### 方式一：apiserver到kubelet
这种连接方式用于：
1. 获取pods的日志
2. 连接(使用kubectl命令行)到运作中的pods
3. 提供kubelet的port-forwarding端口转发功能

这些连接终止与kubelet的HTTPS端口。默认情况下apiserver不会校验kubelet的服务证书，所以有面临中间人攻击的风险，并且在不可信/公共网络中是不安全的。

为了校验这些连接，使用--**kubelet-certificate-authority**给apiserver提供一个根证书bundle，用来验证kubelet的服务证书。

如果上面的方式不行，那么在apiserver和kubelet之间使用SSH隧道，以避免通过不可信或公共网络进行连接。

最后，kubelet身份验证和授权功能应该开启，用于保证kubelet API的安全。

##### 方式二：apiserver到nodes, pods, and services
默认通过HTTP连接，既没有身份验证，也没有加密。可以在API URL里在node、pod、service名前使用https:前缀来使用HTTPS连接，但这种方式不会校验HTTPS端点提供的证书，也不会提供client credentials，因此，虽然连接被加密，但是不会提供任何完整性保证。这样的连接在不可信/公共网络上运行是不安全的。

##### 方式三：SSH tunnels
Kubernetes支持SSH tunnels来保护control plane到节点的通讯。在这种配置中，apiserver启动一个SSH隧道到集群中的每个节点（连接到监听端口22的SSH服务器），并通过隧道传递发送给kubelet、节点、pod或服务的所有流量。此隧道确保通信量不会暴露在运行节点的网络之外。

SSH隧道目前不推荐使用，因此除非您知道自己在做什么，否则您不应该选择使用它们。Konnectivity服务是此通信通道的替代品。

##### 方式四：Konnectivity service
*FEATURE STATE: Kubernetes v1.18 [beta]*
作为SSH隧道的替代，Konnectivity服务为 control plane 到集群的通信提供TCP级别的代理。Konnectivity服务由两部分组成：Konnectivity服务器和Konnectivity代理，分别运行在 control plane 网络和节点网络中。Konnectivity代理启动到Konnectivity服务器的连接并维护网络连接。启用Konnectivity服务后，所有 control plane 到节点的流量都将通过这些连接。


### Controller控制器
在Kubernetes中，控制器是监视集群状态的控制循环，然后在需要时进行或请求更改。每个控制器尝试将当前群集状态移近所需状态。
#### Controller pattern控制形式
控制器可以自己执行操作；更常见的是，在Kubernetes中，控制器将向API服务器发送具有有用副作用的消息。
1. Control via API server
2. Direct control

### Cloud Controller Manager 云控制器管理器
云控制器管理器是一个Kubernetes control plane的组件，它嵌入了特定于云的控制逻辑。云控制器管理器允许您将集群链接到云提供商的API中，并将与该云平台交互的组件与仅与集群交互的组件分离。

通过分离Kubernetes和底层云基础设施之间的互操作性逻辑，云控制器管理器组件使云提供商能够以与主要Kubernetes项目不同的速度发布特性。

云控制器管理器是使用插件机制构建的，该机制允许不同的云提供商将其平台与Kubernetes集成。
#### 云控制器管理器的功能：
1. 节点控制：
    
    初始化新加入的服务器为Node对象、给Node对象上加入特定于云的注释和Label(比如CPU、内存等)、获取节点的主机名和网络地址、校验节点的健康状态、删除不健康节点。

    一些云提供商实现将其分为一个节点控制器和一个单独的节点生命周期控制器

2. Route controller路由控制器
    配置云环境中的路由，从而让不同节点的容器在kubernetes集群中进行通信
3. service controller服务控制器
    服务与云基础设施组件集成，如托管负载平衡器、IP地址、网络包过滤和目标健康检查。当您声明需要负载平衡器和其他基础设施组件的服务资源时，服务控制器与您的云提供商的api交互以设置它们。

#### 授权
本节将分解云控制器管理器对各种API对象的访问要求，以便执行其操作
##### Node controller节点控制器
节点控制器只对Node object执行控制。它需要Node object的完整的读、修改权限

\ | 权限 | 
---------|----------|
 v1/Node | Get、List、Create、Update、Patch、Watch、Delete |

##### Route controller路由控制器
路由控制器监听Node object的创建事件，并适当地配置路由。它需要的权限是：
\ | 权限 | 
---------|----------|
 v1/Node | Get |

##### Service controller服务控制器
服务控制器监听Service object的创建、更新和删除事件，然后适当的配置这些服务的端点Endpoints。它需要的权限有：
\ | 权限 | 
---------|----------|
 v1/Service | List、Get、Watch、Patch、Update |

##### 其他
云控制器管理器核心的实现需要Event object的创建权限，为了确保安全操作，它需要ServiceAccounts的创建权限。需要的权限如下：
\ | 权限 | 
---------|----------|
 v1/Event | Create、Patch、Update |
 v1/ServiceAccount | Create |

Cloud controller manager的RBAC ClusterRole整体如下：
```yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cloud-controller-manager
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - get
  - list
  - watch
  - update
```



## Containers容器

### Runtime Class运行时类
RuntimeClass是用于选择容器运行时配置的功能。容器运行时配置用于运行Pod的容器

#### 动机
您可以在不同的pod之间设置不同的RuntimeClass，以提供性能与安全性的平衡。例如，如果您的一部分工作负载需要高级别的信息安全保证，那么您可以选择安排这些pod，以便它们在使用硬件虚拟化的容器运行时中运行。然后，您将从另一个运行时的额外隔离中获益，但要付出一些额外的开销。

您还可以使用RuntimeClass来运行具有相同容器运行时但具有不同设置的不同pod。

#### Setup
确保RuntimeClass feature gate已启用（默认情况下）。有关启用特性门的说明，请参见特性门。必须在apiservers和kubelets上启用RuntimeClass feature gate。
1. 在节点上配置CRI(Container runtime interface)实现（取决于运行时）
2. 创建相应的RuntimeClass资源

##### 步骤1. 在节点上配置CRI实现
通过RuntimeClass提供的配置依赖于容器运行时接口（CRI）实现。请参阅相应的文档（[下面](https://kubernetes.io/docs/concepts/containers/runtime-class/#cri-configuration)）了解如何配置您的CRI实现。

_默认情况下，RuntimeClass假设集群是同类节点配置（这意味着所有节点的配置方式都与容器运行时相同）。要支持异构节点配置，请参阅[下面的调度](https://kubernetes.io/docs/concepts/containers/runtime-class/#scheduling)_

##### 步骤2. 创建对应的RuntimeClass资源
步骤1中设置的配置都应该有一个关联的handler名，该名称唯一标识这个配置。对于每个handler，创建与之对应的RuntimeClass对象。

RuntimeClass资源当前只有两个有效字段：RuntimeClass名称(metadata.name)以及处理程序（handler）。对象定义如下所示：
```yml
apiVersion: node.k8s.io/v1beta1  # RuntimeClass is defined in the node.k8s.io API group
kind: RuntimeClass
metadata:
  name: myclass  # The name the RuntimeClass will be referenced by
  # RuntimeClass is a non-namespaced resource
handler: myconfiguration  # The name of the corresponding CRI configuration
```


#### 使用RuntimeClass
配置好RuntimeClass后使用非常简单, 使用runtimeClassName指定：
```yml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  runtimeClassName: myclass
  # ...
```
这将指示Kubelet使用命名的RuntimeClass来运行这个pod。如果指定的RuntimeClass不存在，或者CRI无法运行相应的处理程序，pod将进入Failed terminal阶段。查找错误消息的[相应事件](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/)。

如果未指定runtimeClassName，则将使用默认的RuntimeHandler，这相当于禁用RuntimeClass功能时的行为。

##### CRI配置
For more details on setting up CRI runtimes, see [CRI installation](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)


#### Scheduling
_FEATURE STATE: Kubernetes v1.16 [beta]_
从kubernetes v1.16开始，RuntimeClass通过其scheduling字段包含对异构集群的支持。通过使用这些字段，可以确保使用此RuntimeClass运行的pod被调度到支持它的节点上。要使用scheduling支持，必须启用RuntimeClass admission controller（默认值，从1.16开始）。

为了确保Pods在支持特定RuntimeClass的节点上着陆，该组节点应该有一个公共标签，然后被runtimeclass.scheduling.nodeSelector字段选中使用。RuntimeClass的nodeSelector与pod的nodeSelector在admission中合并了，可以有效地获取这两个选择器所选的节点集的交集。如果发生了冲突，pod将被拒绝。

如果受支持的节点受到污染，阻止其他RuntimeClass Pod在该节点上运行，你可以向RuntimeClass添加tolerations。与nodeSelector一样，在admission中RuntimeClass的tolerations和pod的tolerations会被合并，并有效地计算tolerated节点集合的并集。

要了解有关配置node selector和tolerations的更多信息，请参阅[将Pods分配给节点](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

##### Pod Overhead (pod负载)
_FEATURE STATE: Kubernetes v1.18 [beta]_
您可以指定与运行Pod相关联的overhead resources开销资源。声明开销可以让集群（包括调度程序）在决定pod和资源时将所需的overhead因素考虑进来。

要使用Pod overhead，必须启用PodOverhead feature gate（默认情况下是打开的）。

Pod开销是通过overhead字段在RuntimeClass中定义的。通过使用这些字段，您可以指定使用这个RuntimeClass运行pods的开销，并确保这些开销在Kubernetes中得到考虑。

## Container Lifecycle Hooks 容器生命周期钩子
页面描述了kubelet managed Containers如何使用容器生命周期钩子框架来运行管理生命周期中由事件触发的代码。
### Overview
与许多具有组件生命周期钩子的编程语言框架（如Angular）类似，Kubernetes为容器提供了生命周期钩子。钩子使容器能够意识到其管理生命周期中的事件，并在执行相应的生命周期钩子时运行处理程序中实现的代码。
### 容器钩子
容器里有两个暴露的钩子：
1. PostStart
   此钩子在创建容器后立即执行。但是，不能保证钩子将在容器入口点之前执行。没有参数传递给处理程序。
2. PreStop
   由API请求或管理事件（如liveness探测失败、抢占、资源争用等）导致的容器终止之前立即调用此钩子。如果容器已处于终止或已完成状态，则对preStop钩子的调用将失败。它是阻塞的，这意味着它是同步的，因此它必须在发送删除容器的调用之前完成。没有参数传递给处理程序。关于终止行为的更详细的描述可以在[Pods的终止](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)中找到

### 钩子处理程序实现
容器可以通过实现并注册钩子的处理程序来访问该钩子。有两种类型的钩子处理程序可用于容器：
- Exec - 执行特定的命令，例如pre-stop.sh，位于容器的cgroup和命名空间中。命令消耗的资源根据容器计算。
- HTTP - 对容器上的特定端点执行HTTP请求

### 钩子处理程序的执行
当容器生命周期管理钩子调用后，Kubernetes管理系统执行容器中为该钩子注册的处理程序。

容器中的钩子处理程序的调用在包含这个容器的Pod上下文中是同步的。这意味着对于PostStart钩子，容器ENTRYPOINT和钩子异步触发。但是，如果钩子运行时间太长或挂起，容器将无法达到running状态。

这种行为与PreStop钩子类似。如果钩子在执行期间挂起，Pod阶段将保持在terminating状态，并在Pod结束, terminationGracePeriodSeconds之后被终止。如果PostStart或PreStop钩子失败，容器会被杀死。

用户应该使钩子处理程序尽可能轻量。但是，有些情况下，运行一些耗时的长时间命令也是有意义的，例如在停止容器之前保存状态。

### Hook delivery guarantees
钩子传递至少要有一次，这意味着对于任何给定的事件，比如PostStart或PreStop，可能多次调用一个钩子。这就需要钩子的实现正确地处理这种问题。

一般来说，只进行一次delivery传递。例如，如果HTTP钩子接收器关闭并且无法接收通信，则不会尝试重新发送。然而，在一些罕见的情况下，可能会发生双重delivery。例如，如果kubelet在发送钩子的过程中重新启动，那么在kubelet重新启动之后，钩子可能会被重新发送。

### 调试钩子处理程序
钩子处理程序的日志不会在Pod事件中公开。如果处理程序由于某种原因失败，它将广播一个事件。对于PostStart，广播FailedPostStartHook事件；对于PreStop，广播FailedPreStopHook事件。您可以通过运行kubectl describe pod<pod_name>来查看这些事件。以下是运行此命令的事件输出示例：
```bash
Events:
  FirstSeen  LastSeen  Count  From                                                   SubObjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image "test:1.0"
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image "test:1.0"
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to "StartContainer" for "main" with RunContainerError: "PostStart handler: Error executing in Docker Container: 1"
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
```